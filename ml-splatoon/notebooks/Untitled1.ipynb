{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from kedro.framework.context import load_context\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "proj_path = current_dir.parent\n",
    "context = load_context(proj_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-29 07:34:36,619 - kedro.io.data_catalog - INFO - Loading data from `train` (CSVDataSet)...\n",
      "2020-09-29 07:34:36,808 - kedro.io.data_catalog - INFO - Loading data from `test` (CSVDataSet)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\valxa\\anaconda3\\envs\\splatoon-ml\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-29 07:34:36,897 - kedro.io.data_catalog - INFO - Loading data from `stagedata` (CSVDataSet)...\n",
      "2020-09-29 07:34:36,899 - kedro.io.data_catalog - INFO - Loading data from `weapon` (CSVDataSet)...\n"
     ]
    }
   ],
   "source": [
    "df_train = catalog.load(\"train\")\n",
    "df_test = catalog.load(\"test\")\n",
    "df_stage = catalog.load(\"stagedata\")\n",
    "df_weapon = catalog.load(\"weapon\")\n",
    "\n",
    "target = df_train['y']\n",
    "\n",
    "players = [\"A1\", \"A2\", \"A3\", \"A4\", \"B1\", \"B2\", \"B3\", \"B4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['mode', 'stage', 'A1-weapon', 'A2-weapon','A3-weapon','A4-weapon',\n",
    "          'B1-weapon', 'B2-weapon','B3-weapon','B4-weapon']\n",
    "te_cat_cols = ['A1-mainweapon', 'A1-subweapon', 'A1-special', 'A1-category',\n",
    "              'A2-mainweapon', 'A2-subweapon', 'A2-special', 'A2-category',\n",
    "              'A3-mainweapon', 'A3-subweapon', 'A3-special', 'A3-category',\n",
    "              'A4-mainweapon', 'A4-subweapon', 'A4-special', 'A4-category',\n",
    "              'B1-mainweapon', 'B1-subweapon', 'B1-special', 'B1-category',\n",
    "              'B2-mainweapon', 'B2-subweapon', 'B2-special', 'B2-category',\n",
    "              'B3-mainweapon', 'B3-subweapon', 'B3-special', 'B3-category',\n",
    "              'B4-mainweapon', 'B4-subweapon', 'B4-special', 'B4-category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_to_value(x):\n",
    "    rank = {'c-': 1, 'c': 2, 'c+': 3,\n",
    "            'b-': 4, 'b': 5, 'b+': 6,\n",
    "            'a-': 7, 'a': 8, 'a+': 9,\n",
    "            's': 10, 's+': 11, 'x': 12\n",
    "            }\n",
    "    if x in rank:\n",
    "        x = rank[x]\n",
    "    else:\n",
    "        x = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_average(x, team):\n",
    "    x[f\"{team}-rank-ave\"] = (x[f\"{team}1-rank\"] + x[f\"{team}2-rank\"] + x[f\"{team}3-rank\"] + x[f\"{team}4-rank\"]) / 4\n",
    "    return x[f\"{team}-rank-ave\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHot_encoder(x):\n",
    "    import category_encoders as ce\n",
    "    list_cols = ['mode', 'stage']\n",
    "    ce_ohe = ce.OneHotEncoder(cols=list_cols, handle_unknown='impute')\n",
    "    return ce_ohe.fit_transform(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoder(train_x, train_y, test_x, cat_cols):\n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    for c in cat_cols:\n",
    "        data_tmp = pd.DataFrame({c: train_x[c], 'target': train_y})\n",
    "        target_mean = data_tmp.groupby(c)['target'].mean()\n",
    "        \n",
    "        test_x[c] = test_x[c].map(target_mean)\n",
    "        \n",
    "        tmp = np.repeat(np.nan, train_x.shape[0])\n",
    "        \n",
    "        kf = KFold(n_splits=4, shuffle=True, random_state=1234)\n",
    "        \n",
    "        for idx_1, idx_2 in kf.split(train_x):\n",
    "            target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n",
    "            tmp[idx_2] = train_x[c].iloc[idx_2].map(target_mean)\n",
    "        train_x[c] = tmp\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def level_average(x, team):\n",
    "    x[f\"{team}-level-ave\"] = (x[f\"{team}1-level\"] + x[f\"{team}2-level\"] + x[f\"{team}3-level\"] + x[f\"{team}4-level\"]) / 4\n",
    "    return x[f\"{team}-level-ave\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_weapon(weapon):\n",
    "    weapon[\"category\"] = weapon[\"category2\"]\n",
    "    weapon = weapon.drop([\"category1\", \"category2\"], axis=1)\n",
    "    return weapon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stage(stagedata):\n",
    "    return stagedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train, test, stagedata):\n",
    "    data = pd.concat([train, test])\n",
    "    players = [\"A1\", \"A2\", \"A3\", \"A4\", \"B1\", \"B2\", \"B3\", \"B4\"]\n",
    "    data = data.join(stagedata.set_index(\"stage\"), on=\"stage\")\n",
    "    \n",
    "    for player in players:\n",
    "        data[f\"{player}-rank\"] = data[f\"{player}-rank\"].apply(rank_to_value)\n",
    "        data[f\"{player}-weapon\"] = data[f\"{player}-weapon\"].fillna(\"Nothing\")\n",
    "        data[f\"{player}-level\"] = data[f\"{player}-level\"].fillna(0)\n",
    "    data[\"A-rank-ave\"] = rank_average(data, \"A\")\n",
    "    data[\"B-rank-ave\"] = rank_average(data, \"B\")\n",
    "    data[\"A-level-ave\"] = level_average(data, \"A\")\n",
    "    data[\"B-level-ave\"] = level_average(data, \"B\")\n",
    "    data = OneHot_encoder(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_weapon = preprocess_weapon(df_weapon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_stage = preprocess_stage(df_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\valxa\\anaconda3\\envs\\splatoon-ml\\lib\\site-packages\\patsy\\constraint.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "  from collections import Mapping\n",
      "c:\\users\\valxa\\anaconda3\\envs\\splatoon-ml\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "processed_data = preprocess_data(df_train, df_test, df_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\valxa\\anaconda3\\envs\\splatoon-ml\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['id', 'period', 'game-ver', 'lobby-mode', 'lobby', 'mode_1', 'mode_2',\n",
       "       'mode_3', 'mode_4', 'mode_5', 'stage_1', 'stage_2', 'stage_3',\n",
       "       'stage_4', 'stage_5', 'stage_6', 'stage_7', 'stage_8', 'stage_9',\n",
       "       'stage_10', 'stage_11', 'stage_12', 'stage_13', 'stage_14', 'stage_15',\n",
       "       'stage_16', 'stage_17', 'stage_18', 'stage_19', 'stage_20', 'stage_21',\n",
       "       'stage_22', 'stage_23', 'A1-weapon', 'A1-rank', 'A1-level', 'A2-weapon',\n",
       "       'A2-rank', 'A2-level', 'A3-weapon', 'A3-rank', 'A3-level', 'A4-weapon',\n",
       "       'A4-rank', 'A4-level', 'B1-weapon', 'B1-rank', 'B1-level', 'B2-weapon',\n",
       "       'B2-rank', 'B2-level', 'B3-weapon', 'B3-rank', 'B3-level', 'B4-weapon',\n",
       "       'B4-rank', 'B4-level', 'y', 'size', 'A-rank-ave', 'B-rank-ave',\n",
       "       'A-level-ave', 'B-level-ave'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>period</th>\n",
       "      <th>game-ver</th>\n",
       "      <th>lobby-mode</th>\n",
       "      <th>lobby</th>\n",
       "      <th>mode_1</th>\n",
       "      <th>mode_2</th>\n",
       "      <th>mode_3</th>\n",
       "      <th>mode_4</th>\n",
       "      <th>mode_5</th>\n",
       "      <th>...</th>\n",
       "      <th>B3-level</th>\n",
       "      <th>B4-weapon</th>\n",
       "      <th>B4-rank</th>\n",
       "      <th>B4-level</th>\n",
       "      <th>y</th>\n",
       "      <th>size</th>\n",
       "      <th>A-rank-ave</th>\n",
       "      <th>B-rank-ave</th>\n",
       "      <th>A-level-ave</th>\n",
       "      <th>B-level-ave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-15T20:00:00+00:00</td>\n",
       "      <td>5.0.1</td>\n",
       "      <td>regular</td>\n",
       "      <td>standard</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>sharp_neo</td>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2855.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>38.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-12-14T04:00:00+00:00</td>\n",
       "      <td>5.0.1</td>\n",
       "      <td>regular</td>\n",
       "      <td>standard</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>168.0</td>\n",
       "      <td>sputtery_clear</td>\n",
       "      <td>0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2391.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>149.00</td>\n",
       "      <td>130.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-12-25T14:00:00+00:00</td>\n",
       "      <td>5.0.1</td>\n",
       "      <td>gachi</td>\n",
       "      <td>standard</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>160.0</td>\n",
       "      <td>dualsweeper_custom</td>\n",
       "      <td>8</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2426.0</td>\n",
       "      <td>7.75</td>\n",
       "      <td>7.75</td>\n",
       "      <td>128.50</td>\n",
       "      <td>124.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-11-11T14:00:00+00:00</td>\n",
       "      <td>5.0.1</td>\n",
       "      <td>regular</td>\n",
       "      <td>standard</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>194.0</td>\n",
       "      <td>hotblaster_custom</td>\n",
       "      <td>0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2237.4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>174.25</td>\n",
       "      <td>261.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-12-14T06:00:00+00:00</td>\n",
       "      <td>5.0.1</td>\n",
       "      <td>gachi</td>\n",
       "      <td>standard</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>246.0</td>\n",
       "      <td>wakaba</td>\n",
       "      <td>12</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2390.0</td>\n",
       "      <td>12.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>157.00</td>\n",
       "      <td>138.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                     period game-ver lobby-mode     lobby  mode_1  \\\n",
       "0   1  2019-10-15T20:00:00+00:00    5.0.1    regular  standard       1   \n",
       "1   2  2019-12-14T04:00:00+00:00    5.0.1    regular  standard       1   \n",
       "2   3  2019-12-25T14:00:00+00:00    5.0.1      gachi  standard       0   \n",
       "3   4  2019-11-11T14:00:00+00:00    5.0.1    regular  standard       1   \n",
       "4   5  2019-12-14T06:00:00+00:00    5.0.1      gachi  standard       0   \n",
       "\n",
       "   mode_2  mode_3  mode_4  mode_5  ...  B3-level           B4-weapon  B4-rank  \\\n",
       "0       0       0       0       0  ...      68.0           sharp_neo        0   \n",
       "1       0       0       0       0  ...     168.0      sputtery_clear        0   \n",
       "2       1       0       0       0  ...     160.0  dualsweeper_custom        8   \n",
       "3       0       0       0       0  ...     194.0   hotblaster_custom        0   \n",
       "4       1       0       0       0  ...     246.0              wakaba       12   \n",
       "\n",
       "   B4-level    y    size  A-rank-ave  B-rank-ave  A-level-ave  B-level-ave  \n",
       "0      31.0  1.0  2855.0        0.00        0.00        70.00        38.25  \n",
       "1     151.0  0.0  2391.0        0.00        0.00       149.00       130.00  \n",
       "2     126.0  0.0  2426.0        7.75        7.75       128.50       124.75  \n",
       "3     391.0  0.0  2237.4        0.00        0.00       174.25       261.75  \n",
       "4     160.0  1.0  2390.0       12.00       12.00       157.00       138.00  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\valxa\\anaconda3\\envs\\splatoon-ml\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def merge_weapon(data, weapondata):\n",
    "    weapons = [p + \"-weapon\" for p in players]\n",
    "    for weapon in weapons:\n",
    "        temp_weapon_detail = data[[weapon]].join(weapondata.set_index(\"key\"), on=weapon)\n",
    "        weapon_detail = [weapon[:3] + col for col in temp_weapon_detail.columns]\n",
    "        temp_weapon_detail.columns = weapon_detail\n",
    "        data = pd.concat([data, temp_weapon_detail], axis=1)\n",
    "        data = data.drop(weapon[:3] + weapon, axis=1)\n",
    "        data = data.drop(weapon[:3] + 'i', axis=1)\n",
    "        data = data.drop(weapon, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_column(data):\n",
    "    drop_col = [\"id\", \"period\", \"game-ver\", \"lobby-mode\", \"lobby\", ]\n",
    "    data = data.drop(drop_col, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nan(data):\n",
    "    data = data.fillna(0.0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_master_table(data, weapon, stagedata):\n",
    "    master_table = merge_weapon(data, weapon)\n",
    "    master_table = drop_column(master_table)\n",
    "    master_table = process_nan(master_table)\n",
    "    train = master_table.iloc[:len(target), :]\n",
    "    test = master_table.iloc[len(target):, :]\n",
    "    test = test.drop('y', axis=1)\n",
    "    train_x = train.drop('y', axis=1)\n",
    "    train_y = train['y']\n",
    "    target_encoder(train_x, train_y, test, te_cat_cols)\n",
    "    train_x = process_nan(train_x)\n",
    "    test = process_nan(test)\n",
    "    return train_x, train_y, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test = create_master_table(processed_data, processed_weapon, processed_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.isnull().sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mode_1           False\n",
       "mode_2           False\n",
       "mode_3           False\n",
       "mode_4           False\n",
       "mode_5           False\n",
       "stage_1          False\n",
       "stage_2          False\n",
       "stage_3          False\n",
       "stage_4          False\n",
       "stage_5          False\n",
       "stage_6          False\n",
       "stage_7          False\n",
       "stage_8          False\n",
       "stage_9          False\n",
       "stage_10         False\n",
       "stage_11         False\n",
       "stage_12         False\n",
       "stage_13         False\n",
       "stage_14         False\n",
       "stage_15         False\n",
       "stage_16         False\n",
       "stage_17         False\n",
       "stage_18         False\n",
       "stage_19         False\n",
       "stage_20         False\n",
       "stage_21         False\n",
       "stage_22         False\n",
       "stage_23         False\n",
       "A1-rank          False\n",
       "A1-level         False\n",
       "A2-rank          False\n",
       "A2-level         False\n",
       "A3-rank          False\n",
       "A3-level         False\n",
       "A4-rank          False\n",
       "A4-level         False\n",
       "B1-rank          False\n",
       "B1-level         False\n",
       "B2-rank          False\n",
       "B2-level         False\n",
       "B3-rank          False\n",
       "B3-level         False\n",
       "B4-rank          False\n",
       "B4-level         False\n",
       "size             False\n",
       "A-rank-ave       False\n",
       "B-rank-ave       False\n",
       "A-level-ave      False\n",
       "B-level-ave      False\n",
       "A1-subweapon     False\n",
       "A1-special       False\n",
       "A1-mainweapon    False\n",
       "A1-range         False\n",
       "A1-damage        False\n",
       "A1-rate          False\n",
       "A1-category      False\n",
       "A2-subweapon     False\n",
       "A2-special       False\n",
       "A2-mainweapon    False\n",
       "A2-range         False\n",
       "A2-damage        False\n",
       "A2-rate          False\n",
       "A2-category      False\n",
       "A3-subweapon     False\n",
       "A3-special       False\n",
       "A3-mainweapon    False\n",
       "A3-range         False\n",
       "A3-damage        False\n",
       "A3-rate          False\n",
       "A3-category      False\n",
       "A4-subweapon     False\n",
       "A4-special       False\n",
       "A4-mainweapon    False\n",
       "A4-range         False\n",
       "A4-damage        False\n",
       "A4-rate          False\n",
       "A4-category      False\n",
       "B1-subweapon     False\n",
       "B1-special       False\n",
       "B1-mainweapon    False\n",
       "B1-range         False\n",
       "B1-damage        False\n",
       "B1-rate          False\n",
       "B1-category      False\n",
       "B2-subweapon     False\n",
       "B2-special       False\n",
       "B2-mainweapon    False\n",
       "B2-range         False\n",
       "B2-damage        False\n",
       "B2-rate          False\n",
       "B2-category      False\n",
       "B3-subweapon     False\n",
       "B3-special       False\n",
       "B3-mainweapon    False\n",
       "B3-range         False\n",
       "B3-damage        False\n",
       "B3-rate          False\n",
       "B3-category      False\n",
       "B4-subweapon     False\n",
       "B4-special       False\n",
       "B4-mainweapon    False\n",
       "B4-range         False\n",
       "B4-damage        False\n",
       "B4-rate          False\n",
       "B4-category      False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "train_x.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, p=0.5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, out_features, bias)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(CustomLinear(len(train_x.columns), len(train_x.columns)),\n",
    "                   nn.Linear(len(train_x.columns), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, in_features)\n",
    "        self.fc2 = nn.Linear(in_features, out_features)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(len(train_x.columns), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=1234).split(train_x, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\valxa\\anaconda3\\envs\\splatoon-ml\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_epoch = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = np.zeros((len(train_x)))\n",
    "test_preds = np.zeros((len(test)))\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "test_cuda = torch.tensor(test.values, dtype=torch.float32).cuda()\n",
    "test_dataset = torch.utils.data.TensorDataset(test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/6 \t loss=22.1312 \t val_loss=22.1031 \n",
      "Epoch 2/6 \t loss=22.1302 \t val_loss=22.1053 \n",
      "Epoch 3/6 \t loss=22.1313 \t val_loss=22.1032 \n",
      "Epoch 4/6 \t loss=22.1308 \t val_loss=22.1040 \n",
      "Epoch 5/6 \t loss=22.1310 \t val_loss=22.1032 \n",
      "Epoch 6/6 \t loss=22.1309 \t val_loss=22.1040 \n",
      "Fold 2\n",
      "Epoch 1/6 \t loss=22.1306 \t val_loss=22.1034 \n",
      "Epoch 2/6 \t loss=22.1309 \t val_loss=22.1033 \n",
      "Epoch 3/6 \t loss=22.1308 \t val_loss=22.1038 \n",
      "Epoch 4/6 \t loss=22.1313 \t val_loss=22.1034 \n",
      "Epoch 5/6 \t loss=22.1309 \t val_loss=22.1033 \n",
      "Epoch 6/6 \t loss=22.1312 \t val_loss=22.1035 \n",
      "Fold 3\n",
      "Epoch 1/6 \t loss=22.1310 \t val_loss=22.1033 \n",
      "Epoch 2/6 \t loss=22.1306 \t val_loss=22.1038 \n",
      "Epoch 3/6 \t loss=22.1309 \t val_loss=22.1032 \n",
      "Epoch 4/6 \t loss=22.1312 \t val_loss=22.1033 \n",
      "Epoch 5/6 \t loss=22.1304 \t val_loss=22.1036 \n",
      "Epoch 6/6 \t loss=22.1308 \t val_loss=22.1039 \n",
      "Fold 4\n",
      "Epoch 1/6 \t loss=22.1311 \t val_loss=22.1033 \n",
      "Epoch 2/6 \t loss=22.1309 \t val_loss=22.1036 \n",
      "Epoch 3/6 \t loss=22.1310 \t val_loss=22.1032 \n",
      "Epoch 4/6 \t loss=22.1308 \t val_loss=22.1034 \n",
      "Epoch 5/6 \t loss=22.1308 \t val_loss=22.1033 \n",
      "Epoch 6/6 \t loss=22.1311 \t val_loss=22.1033 \n",
      "Fold 5\n",
      "Epoch 1/6 \t loss=22.1312 \t val_loss=22.1033 \n",
      "Epoch 2/6 \t loss=22.1312 \t val_loss=22.1033 \n",
      "Epoch 3/6 \t loss=22.1309 \t val_loss=22.1034 \n",
      "Epoch 4/6 \t loss=22.1311 \t val_loss=22.1033 \n",
      "Epoch 5/6 \t loss=22.1302 \t val_loss=22.1040 \n",
      "Epoch 6/6 \t loss=22.1310 \t val_loss=22.1035 \n"
     ]
    }
   ],
   "source": [
    "for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "    x_train_fold = torch.tensor(train_x.values[train_idx], dtype=torch.float32).cuda()\n",
    "    y_train_fold = torch.tensor(train_y.values[train_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    x_val_fold = torch.tensor(train_x.values[valid_idx], dtype=torch.float32).cuda()\n",
    "    y_val_fold = torch.tensor(train_y.values[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    model = net\n",
    "    model.cuda()\n",
    "    \n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f'Fold {i + 1}')\n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "        model.eval()\n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros(len(test))\n",
    "        avg_val_loss = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            y_pred = model(x_batch).detach()\n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} '.format(\n",
    "            epoch + 1, max_epoch, avg_loss, avg_val_loss))\n",
    "        \n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds += test_preds_fold / len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in [i * 0.01 for i in range(100)]:\n",
    "        score = accuracy_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {\"threshold\": best_threshold, \"accuracy_score\": best_score}\n",
    "    return  search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.0, 'accuracy_score': 0.524703213610586}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result = threshold_search(train_y, train_preds)\n",
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = test_preds > search_result['threshold']\n",
    "search_result['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_iterations': 1000,\n",
    "    'early_stopping_rounds': 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "[LightGBM] [Info] Number of positive: 31248, number of negative: 28264\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6259\n",
      "[LightGBM] [Info] Number of data points in the train set: 59512, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525071 -> initscore=0.100366\n",
      "[LightGBM] [Info] Start training from score 0.100366\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's binary_logloss: 0.634707\tvalid's binary_logloss: 0.683444\n",
      "Early stopping, best iteration is:\n",
      "[43]\ttrain's binary_logloss: 0.660821\tvalid's binary_logloss: 0.683108\n",
      "Fold : 1\n",
      "[LightGBM] [Info] Number of positive: 31254, number of negative: 28258\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6259\n",
      "[LightGBM] [Info] Number of data points in the train set: 59512, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525171 -> initscore=0.100771\n",
      "[LightGBM] [Info] Start training from score 0.100771\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's binary_logloss: 0.635486\tvalid's binary_logloss: 0.682113\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttrain's binary_logloss: 0.646197\tvalid's binary_logloss: 0.681291\n",
      "Fold : 2\n",
      "[LightGBM] [Info] Number of positive: 31171, number of negative: 28341\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6259\n",
      "[LightGBM] [Info] Number of data points in the train set: 59512, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523777 -> initscore=0.095179\n",
      "[LightGBM] [Info] Start training from score 0.095179\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's binary_logloss: 0.63529\tvalid's binary_logloss: 0.686256\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttrain's binary_logloss: 0.667742\tvalid's binary_logloss: 0.68488\n",
      "Fold : 3\n",
      "[LightGBM] [Info] Number of positive: 31213, number of negative: 28299\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6259\n",
      "[LightGBM] [Info] Number of data points in the train set: 59512, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524482 -> initscore=0.098008\n",
      "[LightGBM] [Info] Start training from score 0.098008\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's binary_logloss: 0.634815\tvalid's binary_logloss: 0.685184\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttrain's binary_logloss: 0.657332\tvalid's binary_logloss: 0.683843\n",
      "Fold : 4\n",
      "[LightGBM] [Info] Number of positive: 31256, number of negative: 28256\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6259\n",
      "[LightGBM] [Info] Number of data points in the train set: 59512, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525205 -> initscore=0.100906\n",
      "[LightGBM] [Info] Start training from score 0.100906\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's binary_logloss: 0.636006\tvalid's binary_logloss: 0.683381\n",
      "[200]\ttrain's binary_logloss: 0.597286\tvalid's binary_logloss: 0.684455\n",
      "Early stopping, best iteration is:\n",
      "[133]\ttrain's binary_logloss: 0.622792\tvalid's binary_logloss: 0.68281\n",
      "Fold : 5\n",
      "[LightGBM] [Info] Number of positive: 31170, number of negative: 28343\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6258\n",
      "[LightGBM] [Info] Number of data points in the train set: 59513, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.523751 -> initscore=0.095076\n",
      "[LightGBM] [Info] Start training from score 0.095076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's binary_logloss: 0.635615\tvalid's binary_logloss: 0.682955\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttrain's binary_logloss: 0.659283\tvalid's binary_logloss: 0.682287\n",
      "Fold : 6\n",
      "[LightGBM] [Info] Number of positive: 31226, number of negative: 28287\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007709 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6258\n",
      "[LightGBM] [Info] Number of data points in the train set: 59513, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524692 -> initscore=0.098849\n",
      "[LightGBM] [Info] Start training from score 0.098849\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's binary_logloss: 0.634963\tvalid's binary_logloss: 0.684141\n",
      "Early stopping, best iteration is:\n",
      "[49]\ttrain's binary_logloss: 0.657878\tvalid's binary_logloss: 0.683369\n",
      "Fold : 7\n",
      "[LightGBM] [Info] Number of positive: 31250, number of negative: 28263\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6259\n",
      "[LightGBM] [Info] Number of data points in the train set: 59513, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525095 -> initscore=0.100466\n",
      "[LightGBM] [Info] Start training from score 0.100466\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's binary_logloss: 0.634774\tvalid's binary_logloss: 0.685385\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttrain's binary_logloss: 0.665697\tvalid's binary_logloss: 0.684582\n",
      "Fold : 8\n",
      "[LightGBM] [Info] Number of positive: 31228, number of negative: 28285\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6258\n",
      "[LightGBM] [Info] Number of data points in the train set: 59513, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.524726 -> initscore=0.098983\n",
      "[LightGBM] [Info] Start training from score 0.098983\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's binary_logloss: 0.634766\tvalid's binary_logloss: 0.685712\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttrain's binary_logloss: 0.665574\tvalid's binary_logloss: 0.684323\n",
      "Fold : 9\n",
      "[LightGBM] [Info] Number of positive: 31248, number of negative: 28265\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007918 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6259\n",
      "[LightGBM] [Info] Number of data points in the train set: 59513, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.525062 -> initscore=0.100331\n",
      "[LightGBM] [Info] Start training from score 0.100331\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's binary_logloss: 0.634695\tvalid's binary_logloss: 0.686189\n",
      "Early stopping, best iteration is:\n",
      "[28]\ttrain's binary_logloss: 0.66897\tvalid's binary_logloss: 0.684468\n",
      "\n",
      "################################\n",
      "CV_score:0.5467069620060361\n"
     ]
    }
   ],
   "source": [
    "FOLD_NUM = 10\n",
    "kf = KFold(n_splits=FOLD_NUM, random_state=1234)\n",
    "\n",
    "scores = []\n",
    "\n",
    "pred_cv = np.zeros(len(test.index))\n",
    "\n",
    "num_round = 1000\n",
    "\n",
    "for i, (tdx, vdx) in enumerate(kf.split(train_x, train_y)):\n",
    "    print(f'Fold : {i}')\n",
    "    # 訓練用データと検証用データに分割\n",
    "    X_train, X_valid, y_train, y_valid = train_x.iloc[tdx], train_x.iloc[vdx], train_y.values[tdx], train_y.values[vdx]\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_valid = lgb.Dataset(X_valid, y_valid)\n",
    "    \n",
    "    # 学習の実行\n",
    "    model = lgb.train(params, lgb_train, num_boost_round=num_round,\n",
    "                      valid_names=[\"train\", \"valid\"], valid_sets=[lgb_train, lgb_valid],\n",
    "                      verbose_eval=100)\n",
    "\n",
    "    # 検証データに対する予測値を求めて、勝敗（０　or　１）に変換\n",
    "    va_pred = np.round(model.predict(X_valid,num_iteration=model.best_iteration))\n",
    "    \n",
    "    # accuracyスコアを計算\n",
    "    score_ = accuracy_score(y_valid, va_pred)\n",
    "    \n",
    "    # フォールド毎の検証時のスコアを格納\n",
    "    scores.append(score_)\n",
    "    \n",
    "    #テストデータに対する予測値を求める\n",
    "    submission = model.predict(test,num_iteration=model.best_iteration)\n",
    "    \n",
    "    #テストデータに対する予測値をフォールド数で割って蓄積\n",
    "    #(フォールド毎の予測値の平均値を求めることと同じ)\n",
    "    pred_cv += submission/FOLD_NUM\n",
    "\n",
    "# 最終的なテストデータに対する予測値を勝敗（０　or　１）に変換\n",
    "pred_cv = np.round(pred_cv)\n",
    "\n",
    "# 最終的なaccuracyスコアを平均値で出力\n",
    "print('')\n",
    "print('################################')\n",
    "print('CV_score:'+ str(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出用ファイルを作成する\n",
    "pd.DataFrame({\"id\": range(len(pred_cv)), \"y\": pred_cv }).to_csv(\"../data/07_model_output/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsplatoon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
